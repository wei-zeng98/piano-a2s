<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding | Wei Zeng, Xian He, and Ye Wang National University of Singapore IJCAI 2024</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Wei Zeng, Xian He, and Ye Wang National University of Singapore IJCAI 2024" />
<meta property="og:description" content="Wei Zeng, Xian He, and Ye Wang National University of Singapore IJCAI 2024" />
<link rel="canonical" href="http://localhost:4000/piano-a2s/" />
<meta property="og:url" content="http://localhost:4000/piano-a2s/" />
<meta property="og:site_name" content="End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-10T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","dateModified":"2024-05-10T00:00:00+08:00","datePublished":"2024-05-10T00:00:00+08:00","description":"Wei Zeng, Xian He, and Ye Wang National University of Singapore IJCAI 2024","headline":"End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding","name":"End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding","url":"http://localhost:4000/piano-a2s/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/piano-a2s/assets/css/style.css?v=HEAD">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/piano-a2s/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">End-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding</h1>
      <h2 class="project-tagline">Wei Zeng, Xian He, and Ye Wang <br /> National University of Singapore <br /> IJCAI 2024</h2>
      
        <a href="https://github.com/wei-zeng98/piano-a2s" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="abstract">Abstract</h1>

<p>Piano audio-to-score transcription (A2S) is an important yet underexplored task with extensive applications for music composition, practice, and analysis. However, existing end-to-end piano A2S systems faced difficulties in retrieving bar-level information such as key and time signatures, and have been trained and evaluated with only synthetic data. To address these limitations, we propose a sequence-to-sequence (Seq2Seq) model with a hierarchical decoder that aligns with the hierarchical structure of musical scores, enabling the transcription of score information at both the bar and note levels by multi-task learning. To bridge the gap between synthetic data and recordings of human performance, we propose a two-stage training scheme, which involves pre-training the model using an expressive performance rendering (EPR) system on synthetic audio, followed by fine-tuning the model using recordings of human performance. To preserve the voicing structure for score reconstruction, we propose a pre-processing method for **Kern scores in scenarios with an unconstrained number of voices. Experimental results support the effectiveness of our proposed approaches, in terms of both transcription performance on synthetic audio data in comparison to the current state-of-the-art, and the first experiment on human recordings.</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/wei-zeng98/piano-a2s">piano-a2s</a> is maintained by <a href="https://github.com/wei-zeng98">wei-zeng98</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
